[{"content":"Google Gemini-2.0-Flash-Exp\r2025 年 3 月 12 日，谷歌正式发布 Gemini 2.0 Flash Exp 全模态图像生成器，支持原生图像生成功能，所有开发者都可以通过 Gemini API 和 Google AI Studio 中的实验版本使用 Gemini 2.0 Flash 进行原生图像生成。\n使用方法\u0026amp;费用\rGoogle AI Studio在线使用\rGoogle AI Studio\n按下图所示，右侧选择模型Model为Gemini 2.0 Flash(Image Generation) Experimental Output format选择为Images and text，左侧Create Prompt开始使用 可以先点击3种官方给出的提示词测试看下提示词怎么写 这个翻译插件【沉浸式翻译】比较好用，如下图所示网页上能中英对照等等功能，免费功能够用。 配置Gemini API调用\r按照以下官网几个说明文档\n生成图片 | Gemini API | Google AI for Developers\ncookbook/quickstarts/Image_out.ipynb at main · google-gemini/cookbook\nhttps://ai.google.dev/gemini-api/docs/models/experimental-models?hl=zh-cn#gemini-api\n用AI IDE trae调试了一个脚本，测试成功。\n关键需要装的是这个库pip install google-generativeai 图片数据的处理 脚本如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 from google import genai from google.genai import types from PIL import Image from io import BytesIO import base64 import os from datetime import datetime from dotenv import load_dotenv # 加载环境变量 load_dotenv() # 从环境变量获取API密钥，修改.env.example文件为.env文件并填写YOUR_API_KEY api_key = os.getenv(\u0026#34;GEMINI_API_KEY\u0026#34;) if not api_key: raise ValueError(\u0026#34;请设置环境变量GEMINI_API_KEY或创建.env文件\u0026#34;) # 初始化客户端 client = genai.Client(api_key=api_key) # 设置文本提示 contents = (\u0026#39;Hi, can you create a 3d rendered image of a pig \u0026#39; \u0026#39;with wings and a top hat flying over a happy \u0026#39; \u0026#39;futuristic scifi city with lots of greenery?\u0026#39;) # 生成内容，注意模型名称 response = client.models.generate_content( model=\u0026#34;models/gemini-2.0-flash-exp\u0026#34;, contents=contents, config=types.GenerateContentConfig(response_modalities=[\u0026#39;Text\u0026#39;, \u0026#39;Image\u0026#39;]) ) # 保存生成的图像 save_dir = os.path.join(os.path.dirname(__file__), \u0026#39;GeminiFlash2.0Exp\u0026#39;) os.makedirs(save_dir, exist_ok=True) for part in response.candidates[0].content.parts: if part.text is not None: print(part.text) elif part.inline_data is not None: try: image_data = base64.b64decode(part.inline_data.data) image = Image.open(BytesIO(image_data)) # 生成带时间戳的文件名 timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) image_path = os.path.join(save_dir, f\u0026#39;gemini_image_{timestamp}.png\u0026#39;) # 保存图片 image.save(image_path) print(f\u0026#39;图片已保存至: {image_path}\u0026#39;) # 显示图片 image.show() except Exception as e: print(f\u0026#34;无法处理图像数据: {e}\u0026#34;) print(f\u0026#34;数据类型: {type(part.inline_data.data)}\u0026#34;) 费用\rGemini 2.0 Flash(Image Generation) Exp免费可用，每天免费请求次数1500次。\n记录\r官方提供3种处理样例：\n图片编辑 视觉故事 生日卡片 根据官方的三种样例，结合一些网上视频加了扩展用法做了以下测试\n图片编辑-红烧肉食谱\r视觉故事-小老虎冒险故事\r生日卡片-中英文效果\r扩展-人物一致性-换装、物体\r这是两张原图\n以下两张是Gemini 生成（有一个Gemini AI生成的标识）\n这是更换蓝色T恤成功的 Google的AI生图一向是有意避免生成人脸的；换一个动物来测试下\n扩展-动物-一致性测试\r总结\rGoogle 的 Gemini 2.0 Flash Exp可以实现免费完成图片编辑、生图功能，保持上下文这一点很强，可以解决一些日常基础需求 Stable Diffusion 的WebUI和Comfyui开源免费，当然能更好的实现换装、生图等等，使用门槛太高。 ","date":"2025-03-16T00:00:00Z","image":"http://localhost:1313/p/test-case/GeneratedImageMarch172025-12_46AM_hu_3e679ff8dde16f7.png","permalink":"http://localhost:1313/p/test-case/","title":"Google Gemini-2.0-Flash-Exp Test"},{"content":"一、Github Clone项目镜像\rSpark-TTS项目https://github.com/SparkAudio/Spark-TTS.git\n1 git clone https://github.com/SparkAudio/Spark-TTS.git 1.1 腾讯cloud studio云平台网络代理\r简单说遇到网络不通例如访问Github打开代理，其他网络关闭代理。\n打开代理 1 2 3 4 5 6 7 git config --global http.proxy http://proxy.cloudstudio.work:8081 git config --global https.proxy http://proxy.cloudstudio.work:8081 export http_proxy=http://proxy.cloudstudio.work:8081 export HTTP_PROXY=http://proxy.cloudstudio.work:8081 export https_proxy=http://proxy.cloudstudio.work:8081 export HTTPS_PROXY=http://proxy.cloudstudio.work:8081 关闭代理 1 2 3 4 5 6 git config --global --unset http.proxy git config --global --unset https.proxy unset http_proxy unset HTTP_PROXY unset https_proxy unset HTTPS_PROXY 1.2 PC Github不通的办法\r2025年3月更新！18个Github镜像站，国内更快部署下载\n找到可用镜像站，例如[方达极客社区]https://gitclone.com\n1 2 3 4 5 6 7 8 9 10 11 12 13 方法一（替换URL） git clone https://gitclone.com/github.com/tendermint/tendermint.git 方法二（设置git参数） git config --global url.\u0026#34;https://gitclone.com/\u0026#34;.insteadOf https:// git clone https://github.com/tendermint/tendermint.git 方法三（使用cgit客户端） cgit clone https://github.com/tendermint/tendermint.git 1 git clone https://github.com/SparkAudio/Spark-TTS.git 替换为\n1 git clone https://gitclone.com/github.com/SparkAudio/Spark-TTS.git 二、Conda 环境\r按官方说明，建立python3.12的虚拟环境，安装依赖，\n装依赖使用指定国内源那条命令pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com 1 2 3 4 5 6 conda create -n sparktts -y python=3.12 conda activate sparktts pip install -r requirements.txt # If you are in mainland China, you can set the mirror as follows: pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com 三、模型下载\r官网说明给了两种方式，用其中一种下载就可以。\n3.1 huggingface方式下载\n1 2 3 from huggingface_hub import snapshot_download snapshot_download(\u0026#34;SparkAudio/Spark-TTS-0.5B\u0026#34;, local_dir=\u0026#34;pretrained_models/Spark-TTS-0.5B\u0026#34;) 3.2 git方式\n1 2 3 4 5 6 mkdir -p pretrained_models # Make sure you have git-lfs installed (https://git-lfs.com) git lfs install git clone https://huggingface.co/SparkAudio/Spark-TTS-0.5B pretrained_models/Spark-TTS-0.5B 四、运行测试\r4.1 命令行\r这两种都是服务器上用比较方便，PC这不方便测\nbash 环境\n1 2 cd example bash infer.sh python命令行\n1 2 3 4 5 6 7 python -m cli.inference \\ --text \u0026#34;text to synthesis.\u0026#34; \\ --device 0 \\ --save_dir \u0026#34;path/to/save/audio\u0026#34; \\ --model_dir pretrained_models/Spark-TTS-0.5B \\ --prompt_text \u0026#34;transcript of the prompt audio\u0026#34; \\ --prompt_speech_path \u0026#34;path/to/prompt_audio\u0026#34; 4.2 WebUI\r直接用python跑项目自带的webui.py使用gradio图形化界面测试\n1 2 #python webui.py也可以 python webui.py --device 0 两个功能，Voice Clone是声音克隆；需要提供声音，音频文件或者麦克风录制都可以;官方也提供了很多参考声音以及中英克隆的效果。 详见：SparkTTS-Zero-shot TTS (Voice Cloning) 对应上图的音频如下：\n周杰伦原音-参考音频\n周杰伦声音clone演示\nVoice Creation无参考直接生成\n配置音高Pitch，速度Speed，由模型直接生成声音\n英文P3S3-male-英文 中文P4S2-女声-中文诗词 中文P4S2-男声-中文诗词 中文P5S1-男声-中文诗词 中文P5S2-女声-中文诗词 五、遇到的问题\rPC部署\r容易遇到的pytorch和cuda环境问题\n1 2 3 File \u0026#34;L:\\Users\\yeqing\\anaconda3\\envs\\sparktts\\Lib\\site-packages\\torch\\cuda\\__init__.py\u0026#34;, line 310, in _lazy_init raise AssertionError(\u0026#34;Torch not compiled with CUDA enabled\u0026#34;) AssertionError: Torch not compiled with CUDA enabled 检查了下上述PC操作官方的Git clone下来的requirements.txt里面\n1 torch==2.5.1 pip安装了Pytorch的版本是2.5.1+cpu版本，而非 GPU（CUDA）版本 重新安装一下。CUDA向下兼容PyTorch版本，这样就比较好选了。\n1 2 conda active sparktts pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 另外，PC 本身的NVIDIA CUDA环境查看命令nvcc --version\n1 2 3 4 5 6 7 PS C:\\WINDOWS\\system32\u0026gt; nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2025 NVIDIA Corporation Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025 Cuda compilation tools, release 12.8, V12.8.93 Build cuda_12.8.r12.8/compiler.35583870_0 可以简单弄一个py脚本检查环境env_check.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 import torch #输出True 表示CUDA\\GPU可用 print(\u0026#34;GPU是否可用:\u0026#34;,torch.cuda.is_available()) #如果已安装会输出pytorch 版本号 print(\u0026#34;torch版本:\u0026#34;,torch.__version__) #如果已安装会输出cuda版本号 print(\u0026#34;cuda版本:\u0026#34;,torch.version.cuda) #如果已安装会输出cudnn 版本号 print(\u0026#34;cudnn版本:\u0026#34;,torch.backends.cudnn.version()) 例如正常情况的输出如下\n1 2 3 4 gpu是否可用：True cuda版本: 12.6 torch版本: 2.6.0+cu126 cudnn版本: 90501 云端腾讯Cloud Studio\rgit网络，抱脸模型网络问题，腾讯的代理可解决。\n云端运行Webui，内网穿透问题要用到类似ngrok工具，之前部署ComfyUI用过，实际有点麻烦。 云端用example的infer.sh脚本命令推理或者webui操作服务器下载即可。\nCloud Studio每月免费10000分钟，16G GPU显卡，32G内存，300多G存储，目前是免费可用。单独记录一个文档\n腾讯云服务器Cloud Studio部署SparkTTS\n其他例如CosyVoice、ComfyUI也可以用这个主机部署，16G的GPU虽然对要求高的项目、模型也不够用，但是可以跑一些进行学习试用、帮助了解原理。\n比如ComfyUI的学习，可以正常跑Flux1-dev-fp8模型，出图速度慢些。\n六、总结\r利用好Cloud studio平台，可以学习一些需要GPU的项目，目前高性能AI服务器每月免费10000小时。 例如SprakTTS等Github的开源项目，在Cloud studio很就能部署测试了解了。 SparkTTS，kokoro，CosyVoice，等小型的声音克隆相关项目越来越多，各有特点； 例如kokoro是外国的项目，英文强且速度快几乎能实时转录，有人用作英文教学、英文读物配音；CosyVoice和SparkTTS本身是国产的则对中文支持更友好，CosyVoice带一些音色训练、带有方言、流式推理，3秒极速复刻，支持情绪，部署测试体验差。SparkTTS 克隆效果与CosyVoice类似，推理速度快，整个部署体验更流畅。 ","date":"2025-03-11T00:00:00Z","image":"http://localhost:1313/p/yqsparktts/page_hu_d9d39c68a0ce35e1.png","permalink":"http://localhost:1313/p/yqsparktts/","title":"SparkTTS本地部署"},{"content":"Deepseek R1+Ollama+Dify搭建本地RAG知识库\r注：以下为Windows10系统操作记录，16G内存，NVIDA 4060 8G显卡\nDify\rDify 是低代码一站式大模型应用开发平台。它有可视化界面，降低开发门槛，支持集成多种主流大语言模型。可上传数据微调模型、编排工作流，还能快速部署应用并提供运维监控。支持RAG知识库和AI Agenta 适用于智能客服、内容创作等场景，高效灵活又安全。\n中文官网Dify.AI · 生成式 AI 应用创新引擎\nGithub项目地址Github-Dify\n前置准备\r本文工具和模型\nOllama\u0026mdash;-开源软件，本地部署大模型 Deepseek R1\u0026mdash;- 开源LLM模型 Nomic-Embed-Text\u0026mdash;-开源嵌入向量模型 Dify\u0026mdash;-开源，低代码LLM应用、agent应用平台 Docker Desktop\u0026mdash;Docker官网提供，Windows下运行Docker的工具 Git\u0026mdash;免费开源的分布式版本控制系统，拉取开源项目源码 Windows安装Docker Desktop\r打开Hyper-V 功能\r通过控制面板的方法：\n打开控制面板：可以通过左下方 “开始” 菜单直接搜索 “控制面板” 打开；也可以使用 “Win+R” 快捷键开启运行，输入 “control” 回车进入 “控制面板”。 进入程序和功能：在控制面板中，将查看方式设置为 “大图标” 或 “小图标”，找到并点击 “程序和功能”。 启用或关闭 Windows 功能：在 “程序和功能” 窗口的左侧，点击 “启用或关闭 Windows 功能”。 勾选 Hyper-V：在弹出的 “Windows 功能” 窗口中，找到 “Hyper-V” 选项并勾选，勾选后下方会出现一系列相关的子功能，通常建议全部勾选以确保功能的完整性。 安装并重启：点击 “确定” 后，Windows 将开始下载并安装 Hyper-V 所需的文件，安装完成后，系统会提示需要重启电脑以完成配置。 在 Windows 10 上安装WSL2 以支持安装 Docker\r可参考以下步骤：\n启用适用于 Linux 的 Windows 子系统\r以管理员身份打开 PowerShell，可以通过 “开始” 菜单找到 “PowerShell”，单击右键选择 “以管理员身份运行”。 在 PowerShell 中输入命令：dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart，该命令用于启用适用于 Linux 的 Windows 子系统功能。 启用虚拟机平台\r继续在管理员权限的 PowerShell 中输入：dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart，此命令用于启用虚拟机平台，为 WSL2 提供支持。 输入完上述命令后，重启计算机，以完成 WSL 相关功能的安装和配置。 下载 Linux 内核更新包\r可前往WSL2 Linux 内核更新包下载页面下载适用于 x64 计算机的 WSL2 Linux 内核更新包。\n也可使用wsl.exe --install或wsl.exe --update命令\n验证 WSL2 安装\r在 PowerShell 中输入wsl --list --verbose，可以查看已安装的 WSL 及其版本信息，确认是否成功安装 WSL2。\n安装好 WSL2 后，就可以进行 Docker 的安装。如果要安装 Docker Desktop for Windows，可前往Docker 官方下载页面下载安装程序，然后按照提示完成安装，并在 Docker Desktop 的设置中配置 WSL Integration，勾选安装的 Linux 发行版，以实现 Docker 与 WSL2 的集成。\n实际操作\r以上为一些大模型回答的信息汇总，实际操作时，在WSL2这部分遇到问题，处理解决，一些记录如下\ndism 方式启动虚拟化相关 docker启动后如果有wsl update的报错，需要下载msi文件先安装下\nDocker如果正常跑起来，命令行检查结果\n安装\r采用源码方式安装，需要进行git clone dify源码，docker\ngit clone dify项目源码\r由于不可抗力无法访问Github\n使用国内镜像源，例如使用 git clone https://gitcode.com/gh_mirrors/di/dify.git\n使用docker安装\rclone完成后，命令行进入到dify的docker目录，然后按项目官方说明执行命令进行安装\n1 2 3 4 cd dify cd docker cp .env.example .env docker compose up -d 这里docker compose up这一步会有如下报错，也是因为不可抗力造成无法访问docker的官方镜像仓库地址\n也需要更换国内的镜像仓库解决，例如https://1ms.run 网站提供的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PS C:\\WINDOWS\\system32\u0026gt; docker compose up -d no configuration file provided: not found PS C:\\WINDOWS\\system32\u0026gt; cd L:\\RAG\\Dify\\dify\\docker PS L:\\RAG\\Dify\\dify\\docker\u0026gt; docker compose up -d [+] Running 9/9 ✘ db Error context canceled 17.4s ✘ ssrf_proxy Error context canceled 17.4s ✘ web Error context canceled 17.4s ✘ redis Error context canceled 17.4s ✘ api Error Get \u0026#34;https://registry-1.docker.io/v2/\u0026#34;: dialing registry-1.docker.io:443 contain... 17.4s ✘ weaviate Error context canceled 17.4s ✘ nginx Error context canceled 17.4s ✘ sandbox Error context canceled 17.4s ✘ worker Error context canceled 17.4s Error response from daemon: Get \u0026#34;https://registry-1.docker.io/v2/\u0026#34;: dialing registry-1.docker.io:443 container via direct connection because has no HTTPS proxy: connecting to registry-1.docker.io:443: dial tcp 162.125.32.13:443: connectex: No connection could be made because the target machine actively refused it. PS L:\\RAG\\Dify\\dify\\docker\u0026gt; Windows的docker desktop版本修改镜像仓库地址的方法是直接在图形界面设置中修改，在Docker Engine中增加registry-mirrors配置项，apply\u0026amp;restart重启生效\ndocker info命令检查镜像仓库配置是否生效\n如果镜像仓库可用，修改后可正常安装，如果不能用，还是一样的报错，注意不是因为新配的镜像仓库地址没生效，而是默认和新配的都不可用；\n看上去它这个报错的打印只会显示默认镜像地址的不可用报错。\n安装成功后docker可以看到容器在正常运行\n浏览器访问http://localhost/install进入dify的web界面，配置用户信息。\n设置完后登录正常显示主界面，至此Dify安装成功，可以看到对比anythingLLM界面差距比较大，Dify功能更丰富，工具agent有很多\n知识库功能初测\r进行模型设置，和anythingLLM配置一样，也是选择本地ollama，配置LLM和向量模型，把这两个模型配置好。\n可以看看帮助文档做参考，本文是docker安装方式，所以填http://host.docker.internal:11434\n关于知识库上传文档大小限制，可以根据需要修改dify的docker目录下的.env重启Docker服务生效\n有两处配置需要修改\n上传文件默认15MUPLOAD_FILE_SIZE_LIMIT=15\nnginx的配置客户端上传文件限制15MNGINX_CLIENT_MAX_BODY_SIZE=15M\n1 2 3 4 5 6 # Upload file size limit, default 15M. UPLOAD_FILE_SIZE_LIMIT=15 # Nginx performance tuning NGINX_WORKER_PROCESSES=auto NGINX_CLIENT_MAX_BODY_SIZE=15M NGINX_KEEPALIVE_TIMEOUT=65 知识库创建\r可改为选择推荐混合检索\n聊天应用创建\r测试\r目前能看到是去读取了相关知识库的文件，检查ollama也正常跑了本地的大模型，后续再继续研究回答与AnythingLLM有差异的问题。\n关于Chatbot\r进过调试预览问答后的结果，进行发布更新后，在单独Chat窗口就能得到新发布的结合知识库检索的结果\n文档召回测试\r这里可以输入测试文本在知识库的文档Chunk的命中的情况\n监测功能\r总结\rDify安装使用难点主要在于Windows下Docker Desktop软件问题，以及不可抗力导致的网络连接问题如何不科学上网的情况下解决 Windows 下docker的安装部署，WSL/WSL2相关安装时报错问题 Dify源码安装，拉取Github镜像，会遇到网络问题；docker compose up -d 安装时大概率也会遇到docker镜像仓库不可访问的网络问题 整理了几个网站，在不科学上网情况下，解决上述不可抗力网络问题 Github国内镜像站2025最新可用！18个Github镜像站，国内更快部署下载(1月更新) Docker镜像加速网站服务状态国内 Docker 服务状态 \u0026amp; 镜像加速监控 测试可用网络评价稳定的Docker镜像仓库毫秒镜像 Dify的功能很多，需要更深入的去了解和尝试，目前使用测试来看Dify的可用性，易用性，灵活度，是高于anythingLLM的。 知识库中的文档内容，比如PDF以及文档中的图像，大模型是否分析、处理到了，如何处理图片以及图片上文字的，业务流程图大模型是否能读取分析，需要研究 ","date":"2025-02-04T00:00:00Z","image":"http://localhost:1313/p/rag-dify/page_hu_131ce337008acea9.png","permalink":"http://localhost:1313/p/rag-dify/","title":"Deepseek+Ollama+Dify搭建本地RAG知识库"},{"content":"[TOC]{float}\n音视频转录文字+LLM总结+RAG+工作流自动化\rWin10 8G 显存GPU 为什么使用md格式，了解到向量模型在分段切割时，是对md格式比较友好的，如果是纯txt或者word、PDF格式的文档，切割、分段（块）的效果不好 最终目标，尝试用Dify或其他工具建立工作流，自动化处理音视频转录文字+AI总结+生成文档+保存RAG知识库全套流程 目标\r视频资料的文本提取-\u0026gt;AI总结\u0026gt;知识入库\n对以上进行工作流自动化配置，输入文件\u0026mdash;》最终将核心内容输入知识库（md格式）分块处理，chat问答验证\nOpenAI的whisper开源项目，FFmpge Ollama本地大模型总结/在线大模型API 工作流\u0026mdash;借助Dify或其他工具 Whisper部署\r前置要安装FFmpeg，N卡走GPU需要装CUDA和PyTorch\nFFmpeg安装\r下载安装\r官网-下载页面Download FFmpeg\n选择对应的系统，比如Windows安装，选择其中一个链接，以Whindow builds by BtbN为例，进入后是Github项目的releases页面；\n下载适合本机的Windows后找目录解压即可使用\n配置环境变量\r配置好环境变量方便使用，把bin目录的绝对路径，添加到系统变量的Path里\n测试验证\r检查环境变量配置是否正常，ffmpeg命令有输出说明配置正常，无输出返回上一步检查，bin后有无带\\之类的。\nNVIDIA CUDA工具包\r如果是N卡可以支持GPU方式跑Whisper，需要安装它的CUDA开发工具包\n先检查显卡信息，确认支持CUDA 然后去官网安装CUDA开发者工具包，去官网下载安装，CUDA Toolkit 12.8 Downloads | NVIDIA Developer 选择适合系统的以及安装方式，安装方式\u0026mdash;\u0026gt; local本地安装要下载整个包比较大，network是通过网络安装下载到本地的包小。 下载完毕后进行安装，然后在命令行输入nvcc --version查看是否正常安装了。 有坑，要选择自定义安装，指定装CUDA工具包的路径，不然开始指定的是不对CUDA组件这些生效的还是会被装到C盘去 安装完毕 命令行检查\nWhisper安装\r信息\r官网Introducing Whisper | OpenAI\nGithub地址https://github.com/openai/whisper.git\nHugging Face Whisper项目-右侧)\nHugging Face Space在线演示-Whisper\n麦克风，音频文件，Youtube视频地址三种方式的演示环境\n模型表\r主要看RAM这里，最多尝试到medium和turbo这里\n安装使用\r按照Github项目说明pip安装\n1 pip install -U openai-whisper 太慢了就更换国内源\n1 pip install -U openai-whisper -i https://pypi.tuna.tsinghua.edu.cn/simple 安装完毕即可使用，但是发现是跑的CPU\nwhisper \u0026ndash;help 命令查看帮助\n比如下面这条 指定model 类型为small\n1 whisper test.m4a --model small --language Chinese --model_dir L:\\Whisper 默认走的是CPU，转录的很慢，要走GPU还需要安装CUDA对应版本的Pytouch\n1 whisper test.m4a --model medium --language Chinese --model_dir L:\\Whisper 如果使用的模型第一次使用会先下载模型到本地 需PyTorch才能跑GPU\r安装PyTorch\r到官网Start Locally | PyTorch选择安装对应CUDA版本的版本，官网CUDA目前到126\n1 pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 太慢了，用国内源尝试，还是慢。。。\n1 pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu126 -i https://pypi.tuna.tsinghua.edu.cn/simple 最终安装成功\n注意安装之后是默认走CUDA，走GPU转录\n如果要走CPU ，命令加上-- device cpu，CPU时可以加--threads 8指定线程数\n测试验证\r进入python，用以下命令测试，查看是否正常安装\n1 2 3 import torch print(torch.__version__) print(torch.cuda.is_available()) Whisper+GPU测试\r音频测试\r1 whisper test.m4a --model medium --language Chinese --model_dir L:\\Whisper 如果不是N卡或者显存不够，可以用CPU跑，速度会慢很多。 用GPU的话速度会快一些，不过8G的GPU实测最多也就跑到medium的模型，因为Pytorch会占用内存\n清理了之后，勉强跑起来了，实测效果会比small的模型要好一些，不过也好的不多，见下图\n这是medium的\n这是small的\n视频测试\r1 whisper test.mp4 --model small --language Chinese --model_dir L:\\Whisper 看着是正常能直接透过FFmpeg去解析MP4文件的，默认生成所有格式的，如果只想生成某种格式的文件可以\u0026ndash;output_format命令指定，格式如下\n1 --output_format{txt,vtt,srt,tsv，json,a11} LLM总结内容\r实测Kimi的文章总结能力要比豆包强一些\nDeepSeek R1的结果也比较，而且DeepSeek可以识别tsv格式文件\nGUI工具\r单独文档记录\n做UI界面工具验证接口和相关功能；简化功能验证操作，半自动化，为后续全自动工作流做基础工作。\n转录部分效果 AI处理部分 工作流\r目前已经完成用GUI界面（代码）验证：音视频转录为文本文件 \u0026mdash;\u0026gt; 再用LLM的API接口处理文本文件内容流程\n下一步进行工作流的验证，还在学习中\n","date":"2025-01-01T00:00:00Z","image":"http://localhost:1313/p/yqwhispertest/page_hu_72063d8dd5feea2.png","permalink":"http://localhost:1313/p/yqwhispertest/","title":"AI-Whisper音视频转文字+LLM总结+RAG+工作流.md"},{"content":"[TOC]{float}\n一、登录Cloud Studio创建服务器\r腾讯Cloud Studio\n微信扫码或者PC微信授权登录即可，通用空间每月免费赠送50000分钟，高性能带GPU的每月免费10000分钟 按下图所示选择创建服务器，据说是这个模板GPU和存储空间最优；16G的GPU显存，32G 内存，8核CPU，300G左右存储空间。 二、部署SparkTTS项目\r服务器环境检查\r成功开机后先检查一下服务器环境：\n检查结果：特斯拉T4 显卡，16G GPU；CUDA正常安装； 32G内存，362G存储空间；\n相关命令：\nGPU信息nvidia-smi CUDAnvcc --version\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 (base) root@VM-0-10-ubuntu:/workspace# nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Wed_Jun__8_16:49:14_PDT_2022 Cuda compilation tools, release 11.7, V11.7.99 Build cuda_11.7.r11.7/compiler.31442593_0 (base) root@VM-0-10-ubuntu:/workspace# nvidia-smi Mon Mar 10 05:10:16 2025 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.105.17 Driver Version: 525.105.17 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:09.0 Off | 0 | | N/A 47C P8 14W / 70W | 2MiB / 15360MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 查看内存和存储\n1 2 3 4 5 6 7 (base) root@VM-0-10-ubuntu:/workspace# free -h total used free shared buff/cache available Mem: 30Gi 865Mi 28Gi 2.0Mi 900Mi 29Gi Swap: 0B 0B 0B (base) root@VM-0-10-ubuntu:/workspace# df -h Filesystem Size Used Avail Use% Mounted on overlay 472G 91G 362G 20% / 开始部署\r按照按官网说明文档部署，本地部署文档已经详细说明，不做赘述。\ngit clone项目\n1 2 git clone https://github.com/SparkAudio/Spark-TTS.git cd Spark-TTS conda创建虚拟python环境sparktts，激活虚拟环境，安装项目依赖\n机器自带有安装anaconda 不需要自己安装\n1 2 3 4 5 conda create -n sparktts -y python=3.12 conda activate sparktts pip install -r requirements.txt # 按照官网，国内用下面这条 pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com 测试腾讯云服务器上直接pip install用腾讯自己源非常快。部署时实测确认，哪个快就用那个 新建download_model.py文件并执行下载模型 文件代码： 1 2 3 from huggingface_hub import snapshot_download snapshot_download(\u0026#34;SparkAudio/Spark-TTS-0.5B\u0026#34;, local_dir=\u0026#34;pretrained_models/Spark-TTS-0.5B\u0026#34;) 执行：\n1 python download_model.py 报错ModuleNotFoundError: No module named 'huggingface_hub'的话安装下依赖：\n1 pip install huggingface_hub 上一步安装依赖需要一段时间，这个步骤下载也会花时间，可以和上一步一起并行操作，终端，新建终端操作即可： 注意新建终端后，也要conda active sparktts 避免出现环境问题\n如果运行脚本出现下图的网络报错，那需要配置代理解决，腾讯提供了可用的代理地址1。文章末尾会列出打开和关闭代理命令。 执行打开代理命令，然后重新执行脚本，下载完成 三、测试SparkTTS效果\rsh脚本测试\r直接运行example中的脚本即可测试；\n1 2 cd example bash infer.sh 或者上传参考音频，infer.sh中prompt_text输入参考音频对应的文本，text输入要推理生成声音的文本。 以刘德华声音为例:\n1 2 3 4 promt_text=\u0026#34;所以我觉得这些成功的电影，它都很真诚，而且很有生命力，它就跟当年零号的那个一模一样\u0026#34; text=\u0026#34;所以我觉得好的技术，它一定是有温度的，能够真正服务于大家，而不是冷冰冰地摆在那里。就像现在的语音合成技术，越来越自然，越来越贴近人们的需求，这种发展是有生命力的。它就跟当年那些突破性的科技一样，一开始可能觉得很新鲜，但慢慢地，它就融入生活，变成大家离不开的一部分了。\u0026#34; text=\u0026#34;但是你要记得，成功不是只有一种方法，我离开了电视台，在外面不断拼搏的时候，有一个人留在电视台，那个人就是梁朝伟，今天他的成就，有目共睹，不要胡乱去copy别人成功的例子，一定要想清楚。\u0026#34; promt_speech_path=\u0026#34;example/dehua_promptvn.wav\u0026#34; 参考音频(原音)：\nclone效果： webui测试\r1 python webui.py 1 * Running on local URL: http://0.0.0.0:7860 看到上面这段，然后拼接在自己的主机地址上访问页面，具体方法就是把端口\u0026ndash;7860放在地址栏链接.ap的前面\nhttps://xxxx--7860.ap-shanghai.cloudstudio.work/\n用起来有点问题，页面能按要求推理生成结果，但是没显示也不能下载，只是要去服务器的example/result目录下载文件。 问题应该是需要做内网穿透，用ngrok工具可以（在其他项目试过，有免费次数，官网上使用步骤使用即可，有很详细的说明）； 或者frp（没弄过），这项目页面也凑活能用，就没必要折腾了。\n四、腾讯CloudStudio可用代理\r开启代理 1 2 3 4 5 6 7 git config --global http.proxy http://proxy.cloudstudio.work:8081 git config --global https.proxy http://proxy.cloudstudio.work:8081 export http_proxy=http://proxy.cloudstudio.work:8081 export HTTP_PROXY=http://proxy.cloudstudio.work:8081 export https_proxy=http://proxy.cloudstudio.work:8081 export HTTPS_PROXY=http://proxy.cloudstudio.work:8081 关闭代理 1 2 3 4 5 6 git config --global --unset http.proxy git config --global --unset https.proxy unset http_proxy unset HTTP_PROXY unset https_proxy unset HTTPS_PROXY pip的代理 1 2 3 4 apt-get update apt-get install nano nano /root/.config/pip/pip.conf 腾讯CloudStudio可用代理地址http://proxy.cloudstudio.work:8081 简单来说，遇到下载文件网络不通（常见于外网hugging face、github）打开代理，其他情况关闭代理。 遇到类似问题，切换打开/关闭代理尝试。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-01-01T00:00:00Z","image":"http://localhost:1313/page.png","permalink":"http://localhost:1313/p/yqtesttts/","title":"SparkTTS-腾讯CloudStudio部署"},{"content":"[TOC]{float}\nWhisper GUI简单工具\r目标是验证本地部署Whisper环境部署后，使用Python程序调用的情况。\n功能是否正常 输出结果与命令行方式是否有差异 开发过程\r使用cursor工具，Python语言开发\n新建项目目录（预计功能不多没有初始化Git去管理该项目），Cursor打开，制作好.cursorrules，项目要求不高可直接用相关插件直接生成即可。 梳理项目功能需求逻辑，可用NOTEPADS功能直接记录备用；遇到关键需求，记录到NOTEPADS； 调试测试，处理麦克风初始化、GPU资源占用、释放、界面逻辑优化等等，最终做出符合目标的稳定可用版本；过程省略\n工具使用\r功能主要是音视频文件转录为文字，支持麦克风输入和文件两种方式，详见项目源码目录的README.md\n实际效果如下：\n录音方式\r调用系统麦克风设备，实时转录为文字显示\n文件方式\r支持单个，多个音视频文件（格式未全部测试），进度条显示处理进度。\n结果导入AI总结-在线网站\r这部分应该是包含在程序功能中，例如增加按钮调用AI大模型的API来处理文本结果文件，或是在自动化的工作流中，整体功能还在进行中，未开发完\n先用的网页总结，例如上面的福田AI公务员新闻（注意整个项目的目标和场景是针对长视频而非短视频）： 深圳福田公务员AI总结结果\n功能扩展1\r增加使用API方式的AI总结功能，测试验证\nAI总结-API方式\rAI处理模块功能：通过输入提示词，调用AI大模型（如DeepSeek）进行处理。\n界面输入 API Base URL API Key 模型名称三个关键参数。\nBaseURL支持添加、保存、管理，按名称选择\nAPI KEY 密文显示，不支持存储和管理\n模型名称支持存储并且与API BaseURL关联\n点击\u0026quot;获取模型列表\u0026quot;按钮获取可用模型（发现第三方厂家接口不统一，没必要继续做，目前只支持硅基流动） 选择要处理的转录文本文件，点击\u0026quot;AI处理\u0026quot;按钮即可触发API调用。支持单独提示词交互。支持保存常用提示词\n处理结果将实时显示在状态信息框中。可选流式输出和非流式输出（验证API使用）\n如果选择转录文件，将处理结果保存为同名的md格式文件；如果是未选中转录文本文件，则保存聊天的返回结果，需命名和选择位置。（已实现，已验证，截图略）\n加入环境监测，如果不支持Whisper环境，也可打开程序，支持运行AI处理模块。（已实现，未验证，截图略）\n记录一个Cursor技巧，配置 Cursor 的 @Docs 数据源，可以很方便的将本文档丢给 Cursor 使用。如果文档站支持 llms.txt 协议那效果更好。 以硅基流动的API文档为例：\n配置之后，可以在对话时@Docs 让AI去查看对应的API或其他文档，更准确的开发，例如上述硅基流动的模型获取功能就是如下图所示这样去实现的。 ","date":"2025-01-01T00:00:00Z","image":"http://localhost:1313/p/yeqings/page_hu_44f1c0640b786792.png","permalink":"http://localhost:1313/p/yeqings/","title":"WhisperGUI"},{"content":"正文测试\r而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用\r思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片\r1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"http://localhost:1313/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu_2307260c751d0e0b.jpg","permalink":"http://localhost:1313/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings\rThe following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1\rH2\rH3\rH4\rH5\rH6\rParagraph\rXerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes\rThe blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution\rTiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution\rDon\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables\rTables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables\rItalics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks\rCode block with backticks\r1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces\r\u0026lt;!doctype html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block with Hugo\u0026rsquo;s internal highlight shortcode\r1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block\r1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types\rOrdered List\rFirst item Second item Third item Unordered List\rList item Another item And another item Nested list\rFruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark\rGIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image\rThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"http://localhost:1313/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"http://localhost:1313/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt\rThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat\rVicta caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"http://localhost:1313/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu_c1ca39d792aee4ab.jpg","permalink":"http://localhost:1313/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples\rInline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n$$\r\\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"http://localhost:1313/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"http://localhost:1313/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"http://localhost:1313/p/emoji-support/","title":"Emoji Support"}]