[{"content":"更新日志\rTrae v1.3.0 版本更新，大更新，MCP支持，rules，自定义Agent等\n2025 年 4 月 21 日\rTrae v1.3.0 版本正式发布。以下是变更细节：\n统一对话体验：Chat 与 Builder 面板合并，支持通过 @Builder 进入 Builder Agent 模式。 上下文能力增强：新增支持 #Web 和 #Doc 两种 Context。 #Web：支持联网搜索，可直接粘贴链接，AI 自动提取网页内容作为上下文。 #Doc：支持通过 URL 或上传 .md / .txt 文件添加文档集，最多支持 1000 个文件（50MB）。 自定义规则上线：支持为 Trae 配置个人与项目规则。 个人规则：创建 user_rules.md，跨项目生效。 项目规则：放置于 .trae/rules/project_rules.md，规范当前项目内 AI 行为。 Agent 能力全面升级： 支持通过 prompt 和 tools 自定义 Agent。 内置 Builder Agent 及 Builder with MCP，支持使用历史配置成功的全部 MCP 工具执行复杂任务。 Agent 支持开启“自动运行”模式，自动执行命令和工具调用，支持配置命令黑名单。 MCP 支持上线： 内置 MCP 市场，支持快速添加第三方 MCP Servers。 Agent 可灵活调用 MCP 工具，拓展执行能力。 MCP 功能测试\r从AI侧栏点击右上角①齿轮从AI设置选择MCP进行配置。或者点击②处个人中心图标选择AI功能管理选择MCP进行配置 配置MCP Servers\rTrae的MCP配置中内置了市场方便快速安装MCP Servers，也支持安装市场还没纳入的MCP Servers。\n软件内置市场直接添加MCP Servers 以市场的figma AI Bridge MCP为例，点击右侧+添加，输入API KEY即可添加。 标绿√已经添加成功，可以看到它包含的所有工具 市场还没有的其他MCP Servers 以高德地图为例，接入文档快速接入-MCP Server|高德地图API 点击获取key，获取高德地图的API KEY，使用node.js方式接入，按文档直接复制整段带mcpservers的配置代码。\n点击MCP-\u0026gt;市场-\u0026gt;手动配置，输入上述配置代码。\n可以看到MCP Servers包含的工具集 实测\r使用默认的Build with MCP智能体，默认添加了所有的MCP服务 （测试时使用的MCP是百度地图和腾讯edgeone-pages-mcp-server）\n输入需求测试，部分截图如下： 一轮完成：您可以通过以下链接访问您的天气卡片网页： 乌鲁木齐天气预报 自定义Agent 智能体测试\n从下图标注的3个位置可以进入到智能体配置界面\n创建智能体，进行提示词和智能体使用的MCP Servers等等配置；可以用AI大模型，比如DeepSeek按Trae - 文档提示简单写一个智能体提示词-PromptDeepSeek对话截图(预制的提示是可以不填，估计没有太大作用)\n对话框@新建的智能体，可以看到它只使用配置的2个MCP Servers\n测试：请为我计划一次天津美食一日游。尽量给出更舒适的出行安排，当然，也要注意天气状况。用美观的html网页展示并发布。\n已经生成 发现有些关键数据是联网工具搜索来的，不是走的高德MCP 可以关闭智能体的联网搜索功能，后文也会尝试用新增的rules功能控制测试 rules功能测试\r想要此项目优先使用MCP Server的工具获取数据。进入设置，规则，点击创建项目规则project_rules 测试DeepSeek V3模型可以识别这个rules，在勾选联网的情况下，它优先使用的是MCP工具获取的数据； 可以通过以下链接预览最新效果：杭州美食一日游\n补充\r第一次使用某一个MCP工具会弹出提示框，选择右边运行手动确认，或者勾选左边自动后续程序自动运行。\n智能体配置处也能配置 rules的全局配置 可以配置一些通用配置，比如按提示个人习惯配置 大模型编程能力差异\n大模型编程能力差异对比（本次简单实现HTML网页），可以看到文中三个例子，第一个是用Claude-3.7-Sonnet，第二个是用Claude-3.5-Sonnet，第三个是用的DeepSeekV3-0324 ，提示词都是类似的，从实现效果看，第一个案例效果是最好的，后面两个实际还进行了2-3次优化修改的对话。\n功能 模型 Edgeone Pages网址 天气预报动态卡片网页 Claude-3.7-Sonnet 乌鲁木齐天气预报 天津美食一日游（行程由MCP+联网搜索提供） Claude-3.5-Sonnet 天津美食一日游 - 行程推荐 杭州美食一日游（行程由MCP提供） DeepSeek-V3-0324 杭州美食一日游 总结\rTrae （The Real AI Engineer）现在拥有完整的 AI 编程生态，包含Trae IDE 以及 Trae 插件 Trae这次v1.3.0 的大更新，集成MCP和rules规则配置，自定义智能体，学习了Cursor 、Windsurf 等IDE工具和Cline插件的优点，其中自定义智能体进行了一定创新；字节跳动已经将之前的豆包 MarsCode 编程助手变更为Trae 插件，未来AI编程的国产工具，离不开Trae。\n目前Trae IDE和插件均免费可用，本文使用的是Trae海外版本，热门的Claude-3.7-Sonnet大模型排队更严重了，预计这些本来就费用较高的Claude和ChatGPT闭源大模型会走软件收费订阅处理，离收费不远了。\n","date":"2025-04-22T00:00:00Z","image":"http://localhost:1313/p/trae-v1.3.0/page_hu_227bde73020ce8a5.png","permalink":"http://localhost:1313/p/trae-v1.3.0/","title":"Trae-v1.3.0-大更新MCP实测"},{"content":"MCP学习\r2024年11月25日 Anthropic 公司发布了一种新的开放协议Model Context Protocol ，简称MCP（模型上下文协议）\n2025年3月27日，OpenAI宣布对其Agents SDK进行了重大更新，支持了对手Anthropic推出的MCP服务协议的消息。 OpenAI研究员Steven Heide使用最近OpenAI最火爆GPT-4o的原生图像生成了吉卜力风格图片，解释MCP的架构。 什么是MCP？\r官网Model-Context-Protocol\n官网-Introduction\n模型上下文协议 （MCP） 是一种标准化协议，用于将 AI 代理连接到各种外部工具和数据源。\n可以将 MCP 想象成一个 USB-C 接口，AI 应用通过它以统一的方式连接到各种外部工具和数据源，就像USB-C接口一样，可以用同一根（类型）的数据线将电子设备进行连接，免去用户需要准备多种数据线的烦恼。\n​\t1\n一张动图\r图片实际出处未知，应该是来源于X平台，看到有挺多类似图像\n有无MCP的区别？\r在 MCP 之前，将 AI 模型与各种数据源（文件系统、数据库等）集成需要自定义集成和 API，这既耗时又难以扩展。MCP 提供统一的协议，简化了此过程并改善LLM用户体验。\n​\t2\n如何使用MCP\r工具\rIDE类，claude 当然支持，其他的知名IDE工具也都支持，例如Cursor WindSurf\n插件类，Cline Roo Code Continue 都支持，其中Cline还率先开发了MCP Server Marketplace，相当于MCP server的应用/插件商店，方便安装使用。\n国内AI工具，Cherry Studio， AI 桌面客户端，国产工具更新迭代快，使用简捷，AI相关的各类功能跟进迅速\nMCP Server网站\r网络上各种UP主、博主总结了很多，简单列一些：\nhttps://github.com/modelcontextprotocol/servers Anthropic官方\nhttps://smithery.ai/ 知名社区，收录数量多\nhttps://mcpservers.org/ 简单，分类明确\nMCPServer -Discover Exceptional MCP Servers\nhttps://cursor.directory/ 之前专门收录cursorrules的网站，加入了MCP Server的收集\nhttps://github.com/punkpeye/awesome-mcp-servers Github项目，精选MCPServer\n安装MCP Server\r其实安装除了uv，python，nodejs的基础环境和包安装，主要就是在修改编辑MCP Server相关的json配置文件，各类工具都是基本通用的，先在一个工具上去熟悉安装和修改配置文件，其他的工具是相通的。\n以Cline插件为例，有两种方法，实际源头都是一样，实际都是编辑配置文件\n直接让AI帮你装\r在IDE工具中打开Cline插件（我这使用IDE是trae），选择MCP Servers点击进入Marketplace，找到想要安装的插件，可以搜索或者按Github星标等条件排序。 installed可以看到已经安装的MCP Server\n绿灯表示安装完全没问题，点开可以看到这个MCP包含的所有tools，勾选auto-approve，授权工具自动使用不需要询问。\n比如我这用VSCode Cline重新操作演示安装一个slack 需要前置准备\nslack配置\r注册账号开展工作的平台 | Slack 开通工作空间，进入空间，上面链接T开头的TeamID记录下来 然后在apphttps://api.slack.com/apps/配置这里配置一个机器人\nOAuth \u0026amp; Permissions中按github页面的配置开放所需要的权限；记录Bot User OAuth Token\n把机器人加到频道里\n后续会用到Bot User OAuth Token和TeamID\n点击install，会自动跳到Cline的AI对话，按提示一步步就能生成对应的配置文件内容了，注意Windows电脑要修改下npx这个位置\n其他途径查到的MCP Server安装\r比如地图服务，商店里星标多的Google Map 不适合我们国内使用， 安装一个百度地图。\n首先去百度地图上注册申请API，进行个人认证可以获取到大部分服务每天5000次的额度，个人测试使用足够了。\n点击应用管理，创建应用AK，服务全选，点击AK处的复制，获取完整的AK\n了解百度地图的MCP ServerMCP Server | 百度地图API SDK，使用nodejs方式安装，修改mcp的配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;baidu-map\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@baidumap/mcp-server-baidu-map\u0026#34; ], \u0026#34;env\u0026#34;: { \u0026#34;BAIDU_MAP_API_KEY\u0026#34;: \u0026#34;{您的AK}\u0026#34; } } } } # 实际只用加这一段然后修改下 npx改cmd，下面放npx和/c，填入AK \u0026#34;baidu-map\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;cmd\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;/c\u0026#34;, \u0026#34;npx\u0026#34;, \u0026#34;-y\u0026#34;, \u0026#34;@baidumap/mcp-server-baidu-map\u0026#34; ], \u0026#34;env\u0026#34;: { \u0026#34;BAIDU_MAP_API_KEY\u0026#34;: \u0026#34;{您的AK}\u0026#34; } } 测试\r旅游计划\r结合官方的示例，用地图服务输出一个城市旅游的计划。\n天气测试\r可以从API这里下载一些资源资源下载 | 百度地图API SDK\n例如，用行政区划代码来询问天气，会自动调用baidu-map的MCP，在用到其中的map_weather等等方法去实现输出天气 或者直接用城市名称也是可以的，会比直接行政区划代码，多出一些转换的流程。 多个MCP 结合工作\r把百度地图MCP服务的结果，用slack MCP服务发送到指定的频道 slack服务 其他工具的使用\rCursor\r设置处配置，可以直接把之前的json内容copy过去使用，但是现在Cursor每一个MCP都会有一个程序弹框，体验比较差，可以等后续优化。\nCherryStudio\r这个工具中使用也是类似的选择设置-MCP服务器-编辑MCP配置，把之前的配好的json粘贴过来即可或者重新配置，它也有更多MCP按钮这里方便搜索安装。\njson里加了一个name字段，编辑json时或者界面上编辑加，用于在工具界面上的显示\n按提示安装uvx环境，bun环境\n注意模型配置，需要有函数调用function call功能的模型才可以在cherrystudio中使用MCP。 模型是否支持function call，一般是你添加的时候会直接标识，也可以在官方去确认，像火山引擎的这个v3的250324和R1，加上时没有function call属性实际是支持的，需要配置上这个属性。\n模型配置\rUV和BUN环境配置步骤\r按界面操作指引去安装 这里注意实际大概率是安装失败的，它拉的是github的镜像，有可能提示成功但是实际没安装成功。按安装帮助去操作下。 UV Releases · astral-sh/uv下载对应版本，解压将exe程序放入目录\nBUN用命令安装，拷贝或者软链接到对应目录\n1 powershell -c \u0026#34;irm bun.sh/install.ps1|iex\u0026#34; 最终的cherrystudio相关目录内容如下 测试使用效果\r对话界面选择模型，开启目标MCP服务进行测试\n多个MCP服务协同测试 详细的流程拆解\r可以看到调用了3个MCP工具的多个工具，\n百度地图map_weather查询了天气，map_gecode和map_search_places结合大模型规划了行程 filesystem的create_directory创建目录、write_file写入文件\nslack的工具slack_list_channels slack_plst_messate，查询了频道列表，推送到了slack的work频道 关于OpenRouter API\r另外API可以使用国内可直连的OpenRouter平台，有最新的DeepSeek V3 0324 免费可用\nOpenRouter注册账号，创建API KEY即可， 然后在Cline、ROO Code、CherryStudio等工具里选择厂商，选择free模型使用或者添加使用即可。\n其它\r浏览器，数据库，Git，Github等等，都有相关的MCP Server，总之MCP的种类和能实现的功能非常广泛，根据需求基本都能找得到。 MCP-BrowserTools 浏览器分析-分析网络请求、分析鼠标悬停元素\nMCP-Github1 MCP-Github2 Github操作-仓库查询操作、提交操作\nManus爆火之后一直未正式上线近期公布了其高价收费。除了Github上复刻Manus的项目，字节跳动也发布了开源的Agent TARS项目 https://agent-tars.com/ 目前macOS可用，Windows和其他系统版本还未发布。网站的Blog文章以Agent TARS重点讨论了MCP和AI Agent开发相关内容 Agent TARS -开源多模态 AI Agent-mcp-brings-a-new-paradigm-to-layered-ai-app-development\n总结\r今年AI Agent一直是热点，随着Manus发布带动了MCP全面进入大众视野，通过大模型和MCP可以实现类Manus的智能体；OpenAI作为Anthropic对手公司，官宣支持MCP再次让MCP火爆，目前MCP社区几乎都有2000+的MCP Servers，进行AI学习，有必要对MCP有基础的认知了解，希望本文有所帮助。\nhttps://norahsakal.com/blog/mcp-vs-api-model-context-protocol-explained/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://peerlist.io/zaczuo/articles/understand-mcp-the-emerging-standard-for-agentic-ai\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-03-29T00:00:00Z","image":"http://localhost:1313/p/mcptest/page_hu_2a37fd17ee2b71e8.png","permalink":"http://localhost:1313/p/mcptest/","title":"MCP学习记录"},{"content":"Google Gemini-2.0-Flash-Exp\r2025 年 3 月 12 日，谷歌正式发布 Gemini 2.0 Flash Exp 全模态图像生成器，支持原生图像生成功能，所有开发者都可以通过 Gemini API 和 Google AI Studio 中的实验版本使用 Gemini 2.0 Flash 进行原生图像生成。\n使用方法\u0026amp;费用\rGoogle AI Studio在线使用\rGoogle AI Studio\n按下图所示，右侧选择模型Model为Gemini 2.0 Flash(Image Generation) Experimental Output format选择为Images and text，左侧Create Prompt开始使用 可以先点击3种官方给出的提示词测试看下提示词怎么写 这个翻译插件【沉浸式翻译】比较好用，如下图所示网页上能中英对照等等功能，免费功能够用。 配置Gemini API调用\r按照以下官网几个说明文档\n生成图片 | Gemini API | Google AI for Developers\ncookbook/quickstarts/Image_out.ipynb at main · google-gemini/cookbook\nhttps://ai.google.dev/gemini-api/docs/models/experimental-models?hl=zh-cn#gemini-api\n用AI IDE trae调试了一个脚本，测试成功。\n关键需要装的是这个库pip install google-generativeai 图片数据的处理 脚本如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 from google import genai from google.genai import types from PIL import Image from io import BytesIO import base64 import os from datetime import datetime from dotenv import load_dotenv # 加载环境变量 load_dotenv() # 从环境变量获取API密钥，修改.env.example文件为.env文件并填写YOUR_API_KEY api_key = os.getenv(\u0026#34;GEMINI_API_KEY\u0026#34;) if not api_key: raise ValueError(\u0026#34;请设置环境变量GEMINI_API_KEY或创建.env文件\u0026#34;) # 初始化客户端 client = genai.Client(api_key=api_key) # 设置文本提示 contents = (\u0026#39;Hi, can you create a 3d rendered image of a pig \u0026#39; \u0026#39;with wings and a top hat flying over a happy \u0026#39; \u0026#39;futuristic scifi city with lots of greenery?\u0026#39;) # 生成内容，注意模型名称 response = client.models.generate_content( model=\u0026#34;models/gemini-2.0-flash-exp\u0026#34;, contents=contents, config=types.GenerateContentConfig(response_modalities=[\u0026#39;Text\u0026#39;, \u0026#39;Image\u0026#39;]) ) # 保存生成的图像 save_dir = os.path.join(os.path.dirname(__file__), \u0026#39;GeminiFlash2.0Exp\u0026#39;) os.makedirs(save_dir, exist_ok=True) for part in response.candidates[0].content.parts: if part.text is not None: print(part.text) elif part.inline_data is not None: try: image_data = base64.b64decode(part.inline_data.data) image = Image.open(BytesIO(image_data)) # 生成带时间戳的文件名 timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) image_path = os.path.join(save_dir, f\u0026#39;gemini_image_{timestamp}.png\u0026#39;) # 保存图片 image.save(image_path) print(f\u0026#39;图片已保存至: {image_path}\u0026#39;) # 显示图片 image.show() except Exception as e: print(f\u0026#34;无法处理图像数据: {e}\u0026#34;) print(f\u0026#34;数据类型: {type(part.inline_data.data)}\u0026#34;) 费用\rGemini 2.0 Flash(Image Generation) Exp免费可用，每天免费请求次数1500次。\n记录\r官方提供3种处理样例：\n图片编辑 视觉故事 生日卡片 根据官方的三种样例，结合一些网上视频加了扩展用法做了以下测试\n图片编辑-红烧肉食谱\r视觉故事-小老虎冒险故事\r生日卡片-中英文效果\r扩展-人物一致性-换装、物体\r这是两张原图\n以下两张是Gemini 生成（有一个Gemini AI生成的标识）\n这是更换蓝色T恤成功的 Google的AI生图一向是有意避免生成人脸的；换一个动物来测试下\n扩展-动物-一致性测试\r总结\rGoogle 的 Gemini 2.0 Flash Exp可以实现免费完成图片编辑、生图功能，保持上下文这一点很强，可以解决一些日常基础需求 Stable Diffusion 的WebUI和Comfyui开源免费，当然能更好的实现换装、生图等等，使用门槛太高。 ","date":"2025-03-16T00:00:00Z","image":"http://localhost:1313/p/gemini2.0flashexptest/GeneratedImageMarch172025-12_46AM_hu_3e679ff8dde16f7.png","permalink":"http://localhost:1313/p/gemini2.0flashexptest/","title":"Google Gemini-2.0-Flash-Exp Test"},{"content":"一、Github Clone项目镜像\rSpark-TTS项目https://github.com/SparkAudio/Spark-TTS.git\n1 git clone https://github.com/SparkAudio/Spark-TTS.git 1.1 腾讯cloud studio云平台网络代理\r简单说遇到网络不通例如访问Github打开代理，其他网络关闭代理。\n打开代理 1 2 3 4 5 6 7 git config --global http.proxy http://proxy.cloudstudio.work:8081 git config --global https.proxy http://proxy.cloudstudio.work:8081 export http_proxy=http://proxy.cloudstudio.work:8081 export HTTP_PROXY=http://proxy.cloudstudio.work:8081 export https_proxy=http://proxy.cloudstudio.work:8081 export HTTPS_PROXY=http://proxy.cloudstudio.work:8081 关闭代理 1 2 3 4 5 6 git config --global --unset http.proxy git config --global --unset https.proxy unset http_proxy unset HTTP_PROXY unset https_proxy unset HTTPS_PROXY 1.2 PC Github不通的办法\r2025年3月更新！18个Github镜像站，国内更快部署下载\n找到可用镜像站，例如[方达极客社区]https://gitclone.com\n1 2 3 4 5 6 7 8 9 10 11 12 13 方法一（替换URL） git clone https://gitclone.com/github.com/tendermint/tendermint.git 方法二（设置git参数） git config --global url.\u0026#34;https://gitclone.com/\u0026#34;.insteadOf https:// git clone https://github.com/tendermint/tendermint.git 方法三（使用cgit客户端） cgit clone https://github.com/tendermint/tendermint.git 1 git clone https://github.com/SparkAudio/Spark-TTS.git 替换为\n1 git clone https://gitclone.com/github.com/SparkAudio/Spark-TTS.git 二、Conda 环境\r按官方说明，建立python3.12的虚拟环境，安装依赖，\n装依赖使用指定国内源那条命令pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com 1 2 3 4 5 6 conda create -n sparktts -y python=3.12 conda activate sparktts pip install -r requirements.txt # If you are in mainland China, you can set the mirror as follows: pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com 三、模型下载\r官网说明给了两种方式，用其中一种下载就可以。\n3.1 huggingface方式下载\n1 2 3 from huggingface_hub import snapshot_download snapshot_download(\u0026#34;SparkAudio/Spark-TTS-0.5B\u0026#34;, local_dir=\u0026#34;pretrained_models/Spark-TTS-0.5B\u0026#34;) 3.2 git方式\n1 2 3 4 5 6 mkdir -p pretrained_models # Make sure you have git-lfs installed (https://git-lfs.com) git lfs install git clone https://huggingface.co/SparkAudio/Spark-TTS-0.5B pretrained_models/Spark-TTS-0.5B 四、运行测试\r4.1 命令行\r这两种都是服务器上用比较方便，PC这不方便测\nbash 环境\n1 2 cd example bash infer.sh python命令行\n1 2 3 4 5 6 7 python -m cli.inference \\ --text \u0026#34;text to synthesis.\u0026#34; \\ --device 0 \\ --save_dir \u0026#34;path/to/save/audio\u0026#34; \\ --model_dir pretrained_models/Spark-TTS-0.5B \\ --prompt_text \u0026#34;transcript of the prompt audio\u0026#34; \\ --prompt_speech_path \u0026#34;path/to/prompt_audio\u0026#34; 4.2 WebUI\r直接用python跑项目自带的webui.py使用gradio图形化界面测试\n1 2 #python webui.py也可以 python webui.py --device 0 两个功能，Voice Clone是声音克隆；需要提供声音，音频文件或者麦克风录制都可以;官方也提供了很多参考声音以及中英克隆的效果。 详见：SparkTTS-Zero-shot TTS (Voice Cloning) 对应上图的音频如下：\n周杰伦原音-参考音频\n周杰伦声音clone演示\nVoice Creation无参考直接生成\n配置音高Pitch，速度Speed，由模型直接生成声音\n英文P3S3-male-英文 中文P4S2-女声-中文诗词 中文P4S2-男声-中文诗词 中文P5S1-男声-中文诗词 中文P5S2-女声-中文诗词 五、遇到的问题\rPC部署\r容易遇到的pytorch和cuda环境问题\n1 2 3 File \u0026#34;L:\\Users\\yeqing\\anaconda3\\envs\\sparktts\\Lib\\site-packages\\torch\\cuda\\__init__.py\u0026#34;, line 310, in _lazy_init raise AssertionError(\u0026#34;Torch not compiled with CUDA enabled\u0026#34;) AssertionError: Torch not compiled with CUDA enabled 检查了下上述PC操作官方的Git clone下来的requirements.txt里面\n1 torch==2.5.1 pip安装了Pytorch的版本是2.5.1+cpu版本，而非 GPU（CUDA）版本 重新安装一下。CUDA向下兼容PyTorch版本，这样就比较好选了。\n1 2 conda active sparktts pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 另外，PC 本身的NVIDIA CUDA环境查看命令nvcc --version\n1 2 3 4 5 6 7 PS C:\\WINDOWS\\system32\u0026gt; nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2025 NVIDIA Corporation Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025 Cuda compilation tools, release 12.8, V12.8.93 Build cuda_12.8.r12.8/compiler.35583870_0 可以简单弄一个py脚本检查环境env_check.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 import torch #输出True 表示CUDA\\GPU可用 print(\u0026#34;GPU是否可用:\u0026#34;,torch.cuda.is_available()) #如果已安装会输出pytorch 版本号 print(\u0026#34;torch版本:\u0026#34;,torch.__version__) #如果已安装会输出cuda版本号 print(\u0026#34;cuda版本:\u0026#34;,torch.version.cuda) #如果已安装会输出cudnn 版本号 print(\u0026#34;cudnn版本:\u0026#34;,torch.backends.cudnn.version()) 例如正常情况的输出如下\n1 2 3 4 gpu是否可用：True cuda版本: 12.6 torch版本: 2.6.0+cu126 cudnn版本: 90501 云端腾讯Cloud Studio\rgit网络，抱脸模型网络问题，腾讯的代理可解决。\n云端运行Webui，内网穿透问题要用到类似ngrok工具，之前部署ComfyUI用过，实际有点麻烦。 云端用example的infer.sh脚本命令推理或者webui操作服务器下载即可。\nCloud Studio每月免费10000分钟，16G GPU显卡，32G内存，300多G存储，目前是免费可用。单独记录一个文档\n腾讯云服务器Cloud Studio部署SparkTTS\n其他例如CosyVoice、ComfyUI也可以用这个主机部署，16G的GPU虽然对要求高的项目、模型也不够用，但是可以跑一些进行学习试用、帮助了解原理。\n比如ComfyUI的学习，可以正常跑Flux1-dev-fp8模型，出图速度慢些。\n六、总结\r利用好Cloud studio平台，可以学习一些需要GPU的项目，目前高性能AI服务器每月免费10000小时。 例如SprakTTS等Github的开源项目，在Cloud studio很就能部署测试了解了。 SparkTTS，kokoro，CosyVoice，等小型的声音克隆相关项目越来越多，各有特点； 例如kokoro是外国的项目，英文强且速度快几乎能实时转录，有人用作英文教学、英文读物配音；CosyVoice和SparkTTS本身是国产的则对中文支持更友好，CosyVoice带一些音色训练、带有方言、流式推理，3秒极速复刻，支持情绪，部署测试体验差。SparkTTS 克隆效果与CosyVoice类似，推理速度快，整个部署体验更流畅。 ","date":"2025-03-11T00:00:00Z","image":"http://localhost:1313/p/yqsparktts/page_hu_d9d39c68a0ce35e1.png","permalink":"http://localhost:1313/p/yqsparktts/","title":"SparkTTS本地部署"},{"content":"LM Studio工具记录（含CherryStudio RAG知识库测试）\r下载安装\rLM Studio 官网- Discover, download, and run local LLMs\n官网点击下载安装即可，过程中可选择安装路径（这点比Ollama方便点）\n配置\r中文设置\r右下角齿轮设置处可配置语言为中文\n配置模型存放目录\r默认是C盘，建议修改配置到其他空间充足的盘符目录\n打开Hugging Face代理配置项\r不需魔法，只需要打开这项配置Use LM Studio's Hugging Face Proxy，LM Studio配置的Hugging Face的网络代理，便可以联通Hugging Face下载模型了\n下载模型-软件内-不需魔法\r打开上述配置，可在模型搜索界面直接获取到模型信息，比如下载一个DeepSeek R1 Distill Qwen 7B的模型\nDistill 表示蒸馏模型\n例如DeepSeek R1 Distill Qwen 7B 就是DeepSeek R1 模型对 Qwen-7B 进行蒸馏得到的模型，\n技术特点 蒸馏技术运用：蒸馏是一种模型压缩和优化技术，通过将大模型（教师模型）的知识传递给小模型（学生模型），使小模型在保持一定性能的同时，减少计算量和存储需求，提高运行效率。DeepSeek R1 Distill Qwen 7B 就是利用 DeepSeek R1 的知识对 Qwen-7B 进行蒸馏，让 Qwen-7B 能学习到 DeepSeek R1 的一些优秀特性，如更好的推理能力等。 与原模型关系：它保留了 Qwen-7B 的一些基础架构和特性，同时融入了 DeepSeek R1 的部分优势，是在两者基础上进行融合创新的结果。 下载完毕，正常读取\n检查存放路径，没有问题\n软件内直接使用\r选择模型加载，等待加载完毕后可进行对话，可以从内存随时卸载，另外模型参数可以调整\n下载模型-其他来源-需魔法\r比如从Hugging Face网站下载，需魔法，而且注意要下载GGUF格式的模型，LM Studio才支持。\n左侧栏有模型的类型，搜索、排序找到想要的模型，过滤参数直接加上GGUF\nGGUF的可以直接点USE this model里选LM Studio（弹框确认直接进入LM Studio软件下载），或者直接在文件处点击下载(浏览器下载)\n注意这里下载成功后，需要手动把模型gguf文件放到之前配置的模型目录下的再深入两层的文件夹（自己新建立），才能被识别到，如下图所示：\n实际这两层目录对应在软件上的显示就是，第一层文件夹名对应是发布者这里显示的，第二层文件夹名是对应Model这显示的\nRAG知识库测试\rLM Studio加载好模型\r和Ollama一样，需要LLM模型和embd向量模型\nCherryStudio\r逻辑与使用ollama做本地服务一样，LM Studio作为运行大模型的载体，提供本地模型API服务，CherryStudio、AnythingLLM等客户端工具\n配置模型 配置知识库 新建知识库，导入文件或者文件夹，选择向量模型进行向量化处理，比如选上述LM Studio的向量模型处理，LM开发者界面会显示服务端打印\n对话测试 CherryStudio知识库的缺点是没有列出来具体的引用参考资料文件名（或者带上开关显示引用文件功能更合理）。优点是能直接配置目录，操作简单。\n总结\rLM Studio和Ollama都是本地部署大模型的工具；LM Studio图形化界面，功能更多，操作更简单友好，界面提示准确，遇到问题更容易处理。\n安装配置更友好 如果不装其他客户端直接使用它下载模型本地部署，只使用Chat功能，LM Studio更适合 如果只是用常见模型提供服务，Ollama更适合；如果要使用Hugging Face上更多的模型、量化GGUF模型，LM Studio更适合 追求图形界面操作和简易舒适度，LM Studio更适合 Ollama完全开源，LM Studio核心部分闭源；本地私有化数据，可以通过设置防火墙出入规则来放置LM Studio外漏数据（具体网上可搜到教程） 使用LM Studio几个要点\r上文中都有提到，容易踩坑的点\n模型文件夹配置 打开Hugging Face 代理设置（下载最新版LM Studio软件） 非工具内直接安装，网络单独下载的GGUF模型，需要配置双层文件夹才能被识别到 ","date":"2025-02-17T00:00:00Z","image":"http://localhost:1313/p/lmstudiorag/page_hu_c872e169c28690cf.png","permalink":"http://localhost:1313/p/lmstudiorag/","title":"LM Studio + CherryStudio RAG知识库使用记录"},{"content":"Deepseek R1+Ollama+AnythingLLM搭建本地RAG知识库\r注：以下为Windows10系统操作记录，16G内存，NVIDA 4060 8G显卡\n什么是RAG\rRAG（Retrieval-Augmented Generation）检索增强生成，是一种结合信息检索与生成式语言模型的混合架构，旨在提升生成式任务的事实性、准确性和可解释性。其核心思想是通过动态检索外部知识库中的相关文档片段，为生成模型提供上下文约束，从而缓解传统生成模型的“幻觉”问题，并支持对私有或动态更新的知识源的访问。\nRAG核心流程\r索引（Indexing） 文档分割：把文档库中的长文档拆分成较短的块(Chunk)，每个 Chunk 应包含完整的语义信息，比如一个段落、一个句子等。这样做的目的是为了更精准地匹配用户的问题。 向量索引构建：利用编码器（比如Embedding向量化模型）将每个 Chunk 转换为向量表示，然后将这些向量存储到向量数据库中，构建向量索引，以便后续快速检索。 检索（Retrieval） 相似度计算：将用户提出的问题也转换为向量，然后在向量数据库中计算问题向量与各个 Chunk 向量之间的相似度。 文档片段筛选：根据相似度排序，选取相似度较高的 Chunk 作为相关文档片段。 生成（Generation） 上下文整合：将检索到的相关文档片段作为上下文信息。 答案生成：把问题和上下文信息输入到大语言模型LLM中，让模型根据上下文生成问题的回答 本文工具和模型\nOllama\u0026mdash;-开源软件，本地部署大模型 Deepseek R1\u0026mdash;- 开源LLM模型 Nomic-Embed-Text\u0026mdash;-开源嵌入向量模型 AnythingLLM\u0026mdash;-开源，LLM应用框架 Ollama相关\rOllama 是一个用于本地运行大型语言模型的开源软件\n安装ollama\r官网Ollama点击download进入下载页面选择Windows系统下载安装即可。\nollama安装问题记录\r遇到一个问题，安装程序不能选择安装路径，会默认安装在C盘，Ollama本身占用约5G的空间，下载模型的目录默认也在C盘，会占用很多C盘空间。\n如何安装到其他盘，豆包回复如下\nWindows 系统 使用命令行安装：找到 Ollama 的安装文件，一般是.exe后缀的文件，在命令提示符（CMD）中进入该文件所在目录，然后使用命令.\\\\ollama setup.exe /dir=D:\\\\your desired location，将D:\\\\your desired location替换为你想要安装到的具体路径，如E:\\\\Ollama等，即可将 Ollama 安装到指定盘。 修改环境变量：安装时先让 Ollama 默认安装在 C 盘，安装完成后，打开系统环境变量设置，新增一个变量名为OLLAMA_MODELS，变量值设置为你希望的安装路径，如D:\\\\Ollama。之后重新启动 Ollama 相关进程或命令行工具，这样 Ollama 下载的模型就会存储到指定路径，而不是 C 盘。 实际操作记录\n将ollama程序安装到L盘.\\OllamaSetup.exe /dir=L:/Ollama 安装成功后任务栏会出现程序图标（右键单击图标可退出程序），打开cmd或其他终端命令行输入ollama --version检查\n修改环境变量增加系统变量指定存储模型的路径，变量名 OLLAMA_MODELS变量值 L:\\Ollama\\ollamamodels 注意修改环境变量后，需要重启ollama程序生效\n检查，结果符合预期（结合模型下载步骤）\n模型下载\r本地搭建私有知识库，主要需要两种模型，一类是LLM大语言模型，一类是Embedding向量模型\nOllama 命令如下， 其中run 、pull、 list比较常用\n举例安装一个deepseek-r1:8b模型，一定要有success才是成功，上面那个bge-m3的模型，就是有问题失败了\nLLM模型下载\r点击官网上的models进入选择Deepseek R1进行下载，1.5b的硬件要求最低，本机8G显存理论可跑7b 8b的；具体参考别人的评测\n例如deepseek-r1:1.5b模型，点击复制命令ollama run deepseek-r1:1.5b`在终端命令窗口运行，run命令没有模型会自动安装，pull安装也可以。\nEmbedding模型\r下载步骤也一样，复制命令粘贴执行，选择一个文本处理能力强的体积小的nomic-embed-text模型测试 比如nomic-embed-text模型，复制ollama pull nomic-embed-text\n检查Ollama环境\rollama list查看已下载安装的模型 ollama run deepseek-r1:1.5b 运行此模型，\u0026lt;think\u0026gt; \u0026lt;/think\u0026gt;标签下是R1推理的过程 GPU的情况\r未运行大模型时GPU情况为1.3/8GB\nollama run deepseek-r1:1.5b运行时，2.9/8GB GPU占用约1.6GB\nollama run deepseek-r1:7b运行时，6.1/8.0GB，GPU占用约4.8GB\n本机单靠GPU估计最多也就跑到8b参数，内存只有16G，就不去尝试更大的模型了；\n从结果看7b的回答明显比1.5b的质量有提升，满血的DeepseekR1模型是671b参数，也就是官网勾选深度思考时的回答\nRAG工具\r常见工具对比总结\r表格来源：满血官网DeepseekR1对话，未必准确，参考\n工具 核心能力 适合用户 部署复杂度 扩展性 AnythingLLM 企业级 RAG 框架（含 UI） 需要私有化部署的企业团队 中 高（模块化设计） Dify 低代码 AI 应用开发 非技术团队/快速原型开发 低 中（依赖平台） RAGflow 复杂文档解析与高精度 RAG 技术团队/处理结构化文档场景 高 中 Chatbox 多模型对话客户端 个人开发者/模型测试 低 低 LangChain 灵活编程框架 开发者/定制复杂 AI 逻辑 高 极高 看了网上一些搭建知识库的视频教程，计划如下：\nAnythingLLM部署简单，功能也主要是RAG相关，用于前期搭建知识库试手\nDify的功能强大灵活度高、RAG界面更美观友好、agent应用多，Github星标多且一直在持续增长，后续研究\nanythingLLM\rAnythingLLM 是一个开源的、企业级大语言模型（LLM）应用框架，专为构建私有化、定制化的知识驱动型AI应用而设计。其核心目标是通过灵活的架构支持多种LLM后端（如GPT、Llama、Claude等），并结合检索增强生成（RAG）技术，实现基于私有知识库的精准问答与内容生成。\n安装\r登录官网下载版本安装\n配置\r安装完毕后选择Get started启动，初次会有一些预设的配置，先不选后续也可以修改。 比如可以在LLM Perference选择ollama，会自动读取已经安装的模型\n按步骤设置出工作区\n点击左下方扳手按钮进入设置页，调整模型的配置\nLLM首选项选择之前下载的deepseek-r1:1.5b(前面设置过不用动)\nEmbedder首选项选择之前下载的nomic-embed-text:latest\n其他的比如向量数据库配置，先不修改使用默认的\n测试\r上传本地文档\n点击move to workspace，然后点击save an embed将这些文档进行向量化\n对话测试\n1.5b模型的回答\n7b模型的回答\n总结\r本地搭建私有RAG知识库，数据安全、个性化定制、知识共享方面有明显优势。\n个人私有RAG知识库适用于知识结构化积累和高效检索，适用于学习某一方向的知识记录、工作经验和技能累积等等。\n如果要实现最优的效果，要用到参数很高的大模型版本，对硬件要求很高，个人知识库不需要，企业搭建或定制客户商务项目则需要深度评估投入产出比。\n思考了一些企业的应用场景\n公司内部\r员工日常：将公司制度、办事流程、业务介绍等建立RAG知识库，员工可根据自身需求进行自助了解或查询\n高效知识共享：将全国各个现场问题处理记录、需求\u0026amp;开发技术文档、产品技术文档、运营技术文档、BUG系统记录、任务系统记录等等整合进RAG知识库，员工在遇到问题时更高效的查询相关的信息，想了解某个产品或评估某个需求时能更方便的找到全国的相关信息\n商务合作：将公司成功案例、行业最新资讯等建立RAG知识库，快捷查找到个性化的营销内容\n公司客户\r定制化自助服务： 将日常前线工作人员问题整理成文档进行 RAG 知识库搭建，工作人员可自助问答获取操作指南与故障解决办法，获取个性化的引导服务。 将可公开给目标局方的运维技术文档、故障排查处理记录产品等文档建立RAG知识库，供技术部门或产品部门高效获取目标资源。 其他定制化：根据客户的需求，搭建定制化RAG知识库，或可结合产品、运营内容等进行创新尝试，制造新的AI产品卖点。 ","date":"2025-02-04T00:00:00Z","image":"http://localhost:1313/p/dsollamaanythingllm/page_hu_68f147d1d0fb07ca.png","permalink":"http://localhost:1313/p/dsollamaanythingllm/","title":"Deepseek R1+Ollama+AnythingLLM搭建本地RAG知识库"},{"content":"Deepseek R1+Ollama+Dify搭建本地RAG知识库\r注：以下为Windows10系统操作记录，16G内存，NVIDA 4060 8G显卡\nDify\rDify 是低代码一站式大模型应用开发平台。它有可视化界面，降低开发门槛，支持集成多种主流大语言模型。可上传数据微调模型、编排工作流，还能快速部署应用并提供运维监控。支持RAG知识库和AI Agenta 适用于智能客服、内容创作等场景，高效灵活又安全。\n中文官网Dify.AI · 生成式 AI 应用创新引擎\nGithub项目地址Github-Dify\n前置准备\r本文工具和模型\nOllama\u0026mdash;-开源软件，本地部署大模型 Deepseek R1\u0026mdash;- 开源LLM模型 Nomic-Embed-Text\u0026mdash;-开源嵌入向量模型 Dify\u0026mdash;-开源，低代码LLM应用、agent应用平台 Docker Desktop\u0026mdash;Docker官网提供，Windows下运行Docker的工具 Git\u0026mdash;免费开源的分布式版本控制系统，拉取开源项目源码 Windows安装Docker Desktop\r打开Hyper-V 功能\r通过控制面板的方法：\n打开控制面板：可以通过左下方 “开始” 菜单直接搜索 “控制面板” 打开；也可以使用 “Win+R” 快捷键开启运行，输入 “control” 回车进入 “控制面板”。 进入程序和功能：在控制面板中，将查看方式设置为 “大图标” 或 “小图标”，找到并点击 “程序和功能”。 启用或关闭 Windows 功能：在 “程序和功能” 窗口的左侧，点击 “启用或关闭 Windows 功能”。 勾选 Hyper-V：在弹出的 “Windows 功能” 窗口中，找到 “Hyper-V” 选项并勾选，勾选后下方会出现一系列相关的子功能，通常建议全部勾选以确保功能的完整性。 安装并重启：点击 “确定” 后，Windows 将开始下载并安装 Hyper-V 所需的文件，安装完成后，系统会提示需要重启电脑以完成配置。 在 Windows 10 上安装WSL2 以支持安装 Docker\r可参考以下步骤：\n启用适用于 Linux 的 Windows 子系统\r以管理员身份打开 PowerShell，可以通过 “开始” 菜单找到 “PowerShell”，单击右键选择 “以管理员身份运行”。 在 PowerShell 中输入命令：dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart，该命令用于启用适用于 Linux 的 Windows 子系统功能。 启用虚拟机平台\r继续在管理员权限的 PowerShell 中输入：dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart，此命令用于启用虚拟机平台，为 WSL2 提供支持。 输入完上述命令后，重启计算机，以完成 WSL 相关功能的安装和配置。 下载 Linux 内核更新包\r可前往WSL2 Linux 内核更新包下载页面下载适用于 x64 计算机的 WSL2 Linux 内核更新包。\n也可使用wsl.exe --install或wsl.exe --update命令\n验证 WSL2 安装\r在 PowerShell 中输入wsl --list --verbose，可以查看已安装的 WSL 及其版本信息，确认是否成功安装 WSL2。\n安装好 WSL2 后，就可以进行 Docker 的安装。如果要安装 Docker Desktop for Windows，可前往Docker 官方下载页面下载安装程序，然后按照提示完成安装，并在 Docker Desktop 的设置中配置 WSL Integration，勾选安装的 Linux 发行版，以实现 Docker 与 WSL2 的集成。\n实际操作\r以上为一些大模型回答的信息汇总，实际操作时，在WSL2这部分遇到问题，处理解决，一些记录如下\ndism 方式启动虚拟化相关 docker启动后如果有wsl update的报错，需要下载msi文件先安装下\nDocker如果正常跑起来，命令行检查结果\n安装\r采用源码方式安装，需要进行git clone dify源码，docker\ngit clone dify项目源码\r由于不可抗力无法访问Github\n使用国内镜像源，例如使用 git clone https://gitcode.com/gh_mirrors/di/dify.git\n使用docker安装\rclone完成后，命令行进入到dify的docker目录，然后按项目官方说明执行命令进行安装\n1 2 3 4 cd dify cd docker cp .env.example .env docker compose up -d 这里docker compose up这一步会有如下报错，也是因为不可抗力造成无法访问docker的官方镜像仓库地址\n也需要更换国内的镜像仓库解决，例如https://1ms.run 网站提供的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PS C:\\WINDOWS\\system32\u0026gt; docker compose up -d no configuration file provided: not found PS C:\\WINDOWS\\system32\u0026gt; cd L:\\RAG\\Dify\\dify\\docker PS L:\\RAG\\Dify\\dify\\docker\u0026gt; docker compose up -d [+] Running 9/9 ✘ db Error context canceled 17.4s ✘ ssrf_proxy Error context canceled 17.4s ✘ web Error context canceled 17.4s ✘ redis Error context canceled 17.4s ✘ api Error Get \u0026#34;https://registry-1.docker.io/v2/\u0026#34;: dialing registry-1.docker.io:443 contain... 17.4s ✘ weaviate Error context canceled 17.4s ✘ nginx Error context canceled 17.4s ✘ sandbox Error context canceled 17.4s ✘ worker Error context canceled 17.4s Error response from daemon: Get \u0026#34;https://registry-1.docker.io/v2/\u0026#34;: dialing registry-1.docker.io:443 container via direct connection because has no HTTPS proxy: connecting to registry-1.docker.io:443: dial tcp 162.125.32.13:443: connectex: No connection could be made because the target machine actively refused it. PS L:\\RAG\\Dify\\dify\\docker\u0026gt; Windows的docker desktop版本修改镜像仓库地址的方法是直接在图形界面设置中修改，在Docker Engine中增加registry-mirrors配置项，apply\u0026amp;restart重启生效\ndocker info命令检查镜像仓库配置是否生效\n如果镜像仓库可用，修改后可正常安装，如果不能用，还是一样的报错，注意不是因为新配的镜像仓库地址没生效，而是默认和新配的都不可用；\n看上去它这个报错的打印只会显示默认镜像地址的不可用报错。\n安装成功后docker可以看到容器在正常运行\n浏览器访问http://localhost/install进入dify的web界面，配置用户信息。\n设置完后登录正常显示主界面，至此Dify安装成功，可以看到对比anythingLLM界面差距比较大，Dify功能更丰富，工具agent有很多\n知识库功能初测\r进行模型设置，和anythingLLM配置一样，也是选择本地ollama，配置LLM和向量模型，把这两个模型配置好。\n可以看看帮助文档做参考，本文是docker安装方式，所以填http://host.docker.internal:11434\n关于知识库上传文档大小限制，可以根据需要修改dify的docker目录下的.env重启Docker服务生效\n有两处配置需要修改\n上传文件默认15MUPLOAD_FILE_SIZE_LIMIT=15\nnginx的配置客户端上传文件限制15MNGINX_CLIENT_MAX_BODY_SIZE=15M\n1 2 3 4 5 6 # Upload file size limit, default 15M. UPLOAD_FILE_SIZE_LIMIT=15 # Nginx performance tuning NGINX_WORKER_PROCESSES=auto NGINX_CLIENT_MAX_BODY_SIZE=15M NGINX_KEEPALIVE_TIMEOUT=65 知识库创建\r可改为选择推荐混合检索\n聊天应用创建\r测试\r目前能看到是去读取了相关知识库的文件，检查ollama也正常跑了本地的大模型，后续再继续研究回答与AnythingLLM有差异的问题。\n关于Chatbot\r进过调试预览问答后的结果，进行发布更新后，在单独Chat窗口就能得到新发布的结合知识库检索的结果\n文档召回测试\r这里可以输入测试文本在知识库的文档Chunk的命中的情况\n监测功能\r总结\rDify安装使用难点主要在于Windows下Docker Desktop软件问题，以及不可抗力导致的网络连接问题如何不科学上网的情况下解决 Windows 下docker的安装部署，WSL/WSL2相关安装时报错问题 Dify源码安装，拉取Github镜像，会遇到网络问题；docker compose up -d 安装时大概率也会遇到docker镜像仓库不可访问的网络问题 整理了几个网站，在不科学上网情况下，解决上述不可抗力网络问题 Github国内镜像站2025最新可用！18个Github镜像站，国内更快部署下载(1月更新) Docker镜像加速网站服务状态国内 Docker 服务状态 \u0026amp; 镜像加速监控 测试可用网络评价稳定的Docker镜像仓库毫秒镜像 Dify的功能很多，需要更深入的去了解和尝试，目前使用测试来看Dify的可用性，易用性，灵活度，是高于anythingLLM的。 知识库中的文档内容，比如PDF以及文档中的图像，大模型是否分析、处理到了，如何处理图片以及图片上文字的，业务流程图大模型是否能读取分析，需要研究 ","date":"2025-02-04T00:00:00Z","image":"http://localhost:1313/p/rag-dify/page_hu_131ce337008acea9.png","permalink":"http://localhost:1313/p/rag-dify/","title":"Deepseek+Ollama+Dify搭建本地RAG知识库"},{"content":"[TOC]{float}\n音视频转录文字+LLM总结+RAG+工作流自动化\rWin10 8G 显存GPU 为什么使用md格式，了解到向量模型在分段切割时，是对md格式比较友好的，如果是纯txt或者word、PDF格式的文档，切割、分段（块）的效果不好 最终目标，尝试用Dify或其他工具建立工作流，自动化处理音视频转录文字+AI总结+生成文档+保存RAG知识库全套流程 目标\r视频资料的文本提取-\u0026gt;AI总结\u0026gt;知识入库\n对以上进行工作流自动化配置，输入文件\u0026mdash;》最终将核心内容输入知识库（md格式）分块处理，chat问答验证\nOpenAI的whisper开源项目，FFmpge Ollama本地大模型总结/在线大模型API 工作流\u0026mdash;借助Dify或其他工具 Whisper部署\r前置要安装FFmpeg，N卡走GPU需要装CUDA和PyTorch\nFFmpeg安装\r下载安装\r官网-下载页面Download FFmpeg\n选择对应的系统，比如Windows安装，选择其中一个链接，以Whindow builds by BtbN为例，进入后是Github项目的releases页面；\n下载适合本机的Windows后找目录解压即可使用\n配置环境变量\r配置好环境变量方便使用，把bin目录的绝对路径，添加到系统变量的Path里\n测试验证\r检查环境变量配置是否正常，ffmpeg命令有输出说明配置正常，无输出返回上一步检查，bin后有无带\\之类的。\nNVIDIA CUDA工具包\r如果是N卡可以支持GPU方式跑Whisper，需要安装它的CUDA开发工具包\n先检查显卡信息，确认支持CUDA 然后去官网安装CUDA开发者工具包，去官网下载安装，CUDA Toolkit 12.8 Downloads | NVIDIA Developer 选择适合系统的以及安装方式，安装方式\u0026mdash;\u0026gt; local本地安装要下载整个包比较大，network是通过网络安装下载到本地的包小。 下载完毕后进行安装，然后在命令行输入nvcc --version查看是否正常安装了。 有坑，要选择自定义安装，指定装CUDA工具包的路径，不然开始指定的是不对CUDA组件这些生效的还是会被装到C盘去 安装完毕 命令行检查\nWhisper安装\r信息\r官网Introducing Whisper | OpenAI\nGithub地址https://github.com/openai/whisper.git\nHugging Face Whisper项目-右侧)\nHugging Face Space在线演示-Whisper\n麦克风，音频文件，Youtube视频地址三种方式的演示环境\n模型表\r主要看RAM这里，最多尝试到medium和turbo这里\n安装使用\r按照Github项目说明pip安装\n1 pip install -U openai-whisper 太慢了就更换国内源\n1 pip install -U openai-whisper -i https://pypi.tuna.tsinghua.edu.cn/simple 安装完毕即可使用，但是发现是跑的CPU\nwhisper \u0026ndash;help 命令查看帮助\n比如下面这条 指定model 类型为small\n1 whisper test.m4a --model small --language Chinese --model_dir L:\\Whisper 默认走的是CPU，转录的很慢，要走GPU还需要安装CUDA对应版本的Pytouch\n1 whisper test.m4a --model medium --language Chinese --model_dir L:\\Whisper 如果使用的模型第一次使用会先下载模型到本地 需PyTorch才能跑GPU\r安装PyTorch\r到官网Start Locally | PyTorch选择安装对应CUDA版本的版本，官网CUDA目前到126\n1 pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 太慢了，用国内源尝试，还是慢。。。\n1 pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu126 -i https://pypi.tuna.tsinghua.edu.cn/simple 最终安装成功\n注意安装之后是默认走CUDA，走GPU转录\n如果要走CPU ，命令加上-- device cpu，CPU时可以加--threads 8指定线程数\n测试验证\r进入python，用以下命令测试，查看是否正常安装\n1 2 3 import torch print(torch.__version__) print(torch.cuda.is_available()) Whisper+GPU测试\r音频测试\r1 whisper test.m4a --model medium --language Chinese --model_dir L:\\Whisper 如果不是N卡或者显存不够，可以用CPU跑，速度会慢很多。 用GPU的话速度会快一些，不过8G的GPU实测最多也就跑到medium的模型，因为Pytorch会占用内存\n清理了之后，勉强跑起来了，实测效果会比small的模型要好一些，不过也好的不多，见下图\n这是medium的\n这是small的\n视频测试\r1 whisper test.mp4 --model small --language Chinese --model_dir L:\\Whisper 看着是正常能直接透过FFmpeg去解析MP4文件的，默认生成所有格式的，如果只想生成某种格式的文件可以\u0026ndash;output_format命令指定，格式如下\n1 --output_format{txt,vtt,srt,tsv，json,a11} LLM总结内容\r实测Kimi的文章总结能力要比豆包强一些\nDeepSeek R1的结果也比较，而且DeepSeek可以识别tsv格式文件\nGUI工具\r单独文档记录\n做UI界面工具验证接口和相关功能；简化功能验证操作，半自动化，为后续全自动工作流做基础工作。\n转录部分效果 AI处理部分 工作流\r目前已经完成用GUI界面（代码）验证：音视频转录为文本文件 \u0026mdash;\u0026gt; 再用LLM的API接口处理文本文件内容流程\n下一步进行工作流的验证，还在学习中\n","date":"2025-01-01T00:00:00Z","image":"http://localhost:1313/p/yqwhispertest/page_hu_72063d8dd5feea2.png","permalink":"http://localhost:1313/p/yqwhispertest/","title":"AI-Whisper音视频转文字+LLM总结+RAG+工作流.md"},{"content":"[TOC]{float}\n一、登录Cloud Studio创建服务器\r腾讯Cloud Studio\n微信扫码或者PC微信授权登录即可，通用空间每月免费赠送50000分钟，高性能带GPU的每月免费10000分钟 按下图所示选择创建服务器，据说是这个模板GPU和存储空间最优；16G的GPU显存，32G 内存，8核CPU，300G左右存储空间。 二、部署SparkTTS项目\r服务器环境检查\r成功开机后先检查一下服务器环境：\n检查结果：特斯拉T4 显卡，16G GPU；CUDA正常安装； 32G内存，362G存储空间；\n相关命令：\nGPU信息nvidia-smi CUDAnvcc --version\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 (base) root@VM-0-10-ubuntu:/workspace# nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Wed_Jun__8_16:49:14_PDT_2022 Cuda compilation tools, release 11.7, V11.7.99 Build cuda_11.7.r11.7/compiler.31442593_0 (base) root@VM-0-10-ubuntu:/workspace# nvidia-smi Mon Mar 10 05:10:16 2025 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.105.17 Driver Version: 525.105.17 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:09.0 Off | 0 | | N/A 47C P8 14W / 70W | 2MiB / 15360MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 查看内存和存储\n1 2 3 4 5 6 7 (base) root@VM-0-10-ubuntu:/workspace# free -h total used free shared buff/cache available Mem: 30Gi 865Mi 28Gi 2.0Mi 900Mi 29Gi Swap: 0B 0B 0B (base) root@VM-0-10-ubuntu:/workspace# df -h Filesystem Size Used Avail Use% Mounted on overlay 472G 91G 362G 20% / 开始部署\r按照按官网说明文档部署，本地部署文档已经详细说明，不做赘述。\ngit clone项目\n1 2 git clone https://github.com/SparkAudio/Spark-TTS.git cd Spark-TTS conda创建虚拟python环境sparktts，激活虚拟环境，安装项目依赖\n机器自带有安装anaconda 不需要自己安装\n1 2 3 4 5 conda create -n sparktts -y python=3.12 conda activate sparktts pip install -r requirements.txt # 按照官网，国内用下面这条 pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com 测试腾讯云服务器上直接pip install用腾讯自己源非常快。部署时实测确认，哪个快就用那个 新建download_model.py文件并执行下载模型 文件代码： 1 2 3 from huggingface_hub import snapshot_download snapshot_download(\u0026#34;SparkAudio/Spark-TTS-0.5B\u0026#34;, local_dir=\u0026#34;pretrained_models/Spark-TTS-0.5B\u0026#34;) 执行：\n1 python download_model.py 报错ModuleNotFoundError: No module named 'huggingface_hub'的话安装下依赖：\n1 pip install huggingface_hub 上一步安装依赖需要一段时间，这个步骤下载也会花时间，可以和上一步一起并行操作，终端，新建终端操作即可： 注意新建终端后，也要conda active sparktts 避免出现环境问题\n如果运行脚本出现下图的网络报错，那需要配置代理解决，腾讯提供了可用的代理地址1。文章末尾会列出打开和关闭代理命令。 执行打开代理命令，然后重新执行脚本，下载完成 三、测试SparkTTS效果\rsh脚本测试\r直接运行example中的脚本即可测试；\n1 2 cd example bash infer.sh 或者上传参考音频，infer.sh中prompt_text输入参考音频对应的文本，text输入要推理生成声音的文本。 以刘德华声音为例:\n1 2 3 4 promt_text=\u0026#34;所以我觉得这些成功的电影，它都很真诚，而且很有生命力，它就跟当年零号的那个一模一样\u0026#34; text=\u0026#34;所以我觉得好的技术，它一定是有温度的，能够真正服务于大家，而不是冷冰冰地摆在那里。就像现在的语音合成技术，越来越自然，越来越贴近人们的需求，这种发展是有生命力的。它就跟当年那些突破性的科技一样，一开始可能觉得很新鲜，但慢慢地，它就融入生活，变成大家离不开的一部分了。\u0026#34; text=\u0026#34;但是你要记得，成功不是只有一种方法，我离开了电视台，在外面不断拼搏的时候，有一个人留在电视台，那个人就是梁朝伟，今天他的成就，有目共睹，不要胡乱去copy别人成功的例子，一定要想清楚。\u0026#34; promt_speech_path=\u0026#34;example/dehua_promptvn.wav\u0026#34; 参考音频(原音)：\nclone效果： webui测试\r1 python webui.py 1 * Running on local URL: http://0.0.0.0:7860 看到上面这段，然后拼接在自己的主机地址上访问页面，具体方法就是把端口\u0026ndash;7860放在地址栏链接.ap的前面\nhttps://xxxx--7860.ap-shanghai.cloudstudio.work/\n用起来有点问题，页面能按要求推理生成结果，但是没显示也不能下载，只是要去服务器的example/result目录下载文件。 问题应该是需要做内网穿透，用ngrok工具可以（在其他项目试过，有免费次数，官网上使用步骤使用即可，有很详细的说明）； 或者frp（没弄过），这项目页面也凑活能用，就没必要折腾了。\n四、腾讯CloudStudio可用代理\r开启代理 1 2 3 4 5 6 7 git config --global http.proxy http://proxy.cloudstudio.work:8081 git config --global https.proxy http://proxy.cloudstudio.work:8081 export http_proxy=http://proxy.cloudstudio.work:8081 export HTTP_PROXY=http://proxy.cloudstudio.work:8081 export https_proxy=http://proxy.cloudstudio.work:8081 export HTTPS_PROXY=http://proxy.cloudstudio.work:8081 关闭代理 1 2 3 4 5 6 git config --global --unset http.proxy git config --global --unset https.proxy unset http_proxy unset HTTP_PROXY unset https_proxy unset HTTPS_PROXY pip的代理 1 2 3 4 apt-get update apt-get install nano nano /root/.config/pip/pip.conf 腾讯CloudStudio可用代理地址http://proxy.cloudstudio.work:8081 简单来说，遇到下载文件网络不通（常见于外网hugging face、github）打开代理，其他情况关闭代理。 遇到类似问题，切换打开/关闭代理尝试。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-01-01T00:00:00Z","permalink":"http://localhost:1313/p/yqtesttts/","title":"SparkTTS-腾讯CloudStudio部署"},{"content":"[TOC]{float}\nWhisper GUI简单工具\r目标是验证本地部署Whisper环境部署后，使用Python程序调用的情况。\n功能是否正常 输出结果与命令行方式是否有差异 开发过程\r使用cursor工具，Python语言开发\n新建项目目录（预计功能不多没有初始化Git去管理该项目），Cursor打开，制作好.cursorrules，项目要求不高可直接用相关插件直接生成即可。 梳理项目功能需求逻辑，可用NOTEPADS功能直接记录备用；遇到关键需求，记录到NOTEPADS； 调试测试，处理麦克风初始化、GPU资源占用、释放、界面逻辑优化等等，最终做出符合目标的稳定可用版本；过程省略\n工具使用\r功能主要是音视频文件转录为文字，支持麦克风输入和文件两种方式，详见项目源码目录的README.md\n实际效果如下：\n录音方式\r调用系统麦克风设备，实时转录为文字显示\n文件方式\r支持单个，多个音视频文件（格式未全部测试），进度条显示处理进度。\n结果导入AI总结-在线网站\r这部分应该是包含在程序功能中，例如增加按钮调用AI大模型的API来处理文本结果文件，或是在自动化的工作流中，整体功能还在进行中，未开发完\n先用的网页总结，例如上面的福田AI公务员新闻（注意整个项目的目标和场景是针对长视频而非短视频）： 深圳福田公务员AI总结结果\n功能扩展1\r增加使用API方式的AI总结功能，测试验证\nAI总结-API方式\rAI处理模块功能：通过输入提示词，调用AI大模型（如DeepSeek）进行处理。\n界面输入 API Base URL API Key 模型名称三个关键参数。\nBaseURL支持添加、保存、管理，按名称选择\nAPI KEY 密文显示，不支持存储和管理\n模型名称支持存储并且与API BaseURL关联\n点击\u0026quot;获取模型列表\u0026quot;按钮获取可用模型（发现第三方厂家接口不统一，没必要继续做，目前只支持硅基流动） 选择要处理的转录文本文件，点击\u0026quot;AI处理\u0026quot;按钮即可触发API调用。支持单独提示词交互。支持保存常用提示词\n处理结果将实时显示在状态信息框中。可选流式输出和非流式输出（验证API使用）\n如果选择转录文件，将处理结果保存为同名的md格式文件；如果是未选中转录文本文件，则保存聊天的返回结果，需命名和选择位置。（已实现，已验证，截图略）\n加入环境监测，如果不支持Whisper环境，也可打开程序，支持运行AI处理模块。（已实现，未验证，截图略）\n记录一个Cursor技巧，配置 Cursor 的 @Docs 数据源，可以很方便的将本文档丢给 Cursor 使用。如果文档站支持 llms.txt 协议那效果更好。 以硅基流动的API文档为例：\n配置之后，可以在对话时@Docs 让AI去查看对应的API或其他文档，更准确的开发，例如上述硅基流动的模型获取功能就是如下图所示这样去实现的。 ","date":"2025-01-01T00:00:00Z","image":"http://localhost:1313/p/yeqings/page_hu_44f1c0640b786792.png","permalink":"http://localhost:1313/p/yeqings/","title":"WhisperGUI"},{"content":"正文测试\r而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用\r思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片\r1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"http://localhost:1313/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu_2307260c751d0e0b.jpg","permalink":"http://localhost:1313/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings\rThe following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1\rH2\rH3\rH4\rH5\rH6\rParagraph\rXerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes\rThe blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution\rTiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution\rDon\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables\rTables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables\rItalics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks\rCode block with backticks\r1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces\r\u0026lt;!doctype html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block with Hugo\u0026rsquo;s internal highlight shortcode\r1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block\r1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types\rOrdered List\rFirst item Second item Third item Unordered List\rList item Another item And another item Nested list\rFruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark\rGIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image\rThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"http://localhost:1313/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"http://localhost:1313/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt\rThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat\rVicta caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"http://localhost:1313/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu_c1ca39d792aee4ab.jpg","permalink":"http://localhost:1313/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples\rInline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n$$\r\\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"http://localhost:1313/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"http://localhost:1313/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"http://localhost:1313/p/emoji-support/","title":"Emoji Support"}]